<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>有限元学习笔记01</title>
      <link href="/2025/11/10/FEM_note1/"/>
      <url>/2025/11/10/FEM_note1/</url>
      
        <content type="html"><![CDATA[<p>因为PINN的学习需要，所以重新捡起了有限元的学习。<br>在本章中我们介绍一种称为分段多项式的函数，用它来近似很多通用函数，并很容易在计算机软件中实现。<br>为了分段多项式近似，我们提出了两种技术：插值和L2投影。我们还证明了这些近似值的误差估计。</p><span id="more"></span><h2 id="分段多项式空间（Piecewise-Polynomial-Spaces）"><a href="#分段多项式空间（Piecewise-Polynomial-Spaces）" class="headerlink" title="分段多项式空间（Piecewise Polynomial Spaces）"></a>分段多项式空间（Piecewise Polynomial Spaces）</h2><h3 id="线性多项式空间（The-Space-of-Linear-Polynomials）"><a href="#线性多项式空间（The-Space-of-Linear-Polynomials）" class="headerlink" title="线性多项式空间（The Space of Linear Polynomials）"></a>线性多项式空间（The Space of Linear Polynomials）</h3><p>令$I&#x3D;[x_0,x_1]$是实轴上的一个区间，并令$P_1(I)$代表$I$上线性函数的向量空间，定义为：<br>$$<br>P_1(I)&#x3D;{v:v(x)&#x3D;c_0+c_1x,x\in I,c_0,c_1\in \mathbb{R}}.\tag{1.1}<br>$$<br>也就是说，$P_1(I)$包含了$I$上所有形式为$v(x)&#x3D;c_0+c_1x$的函数。  </p><p>可能$P_1(I)$ 最自然的基是单项式基底 ${1,x}$。因为$P_1(I)$中的任何函数$v$都可以表示为1和$x$的线性组合。也就是说，一个常数$c_0$乘1加上另一个常数$c_1$乘$x$，在这样做的过程中，$v$很清晰的被两个系数$c_0$和$c_1$（所谓的线性组合系数）所决定。事实上，我们称：<strong>$v$有两个自由度</strong></p>]]></content>
      
      
      <categories>
          
          <category> 有限元学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FEM </tag>
            
            <tag> 数值分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>离线超算机器学习环境配置</title>
      <link href="/2025/10/31/offline_env001/"/>
      <url>/2025/10/31/offline_env001/</url>
      
        <content type="html"><![CDATA[<p>由于超算无法连接到网上环境，通过pip 或者conda 的直接下载包的方式失效。以下提供几种方式。主流的方式应该是在本地docker  创建一个镜像，直接在超算运行镜像环境，这种方法是运行环境最稳定的，缺点是docker的安装和学习 需要一些成本，同时docker hub的国内链接受到网络环境影响，很难下载镜像，在2024.11.7我曾试图在超算上下载image自己改环境，在下载到5gb大小的时候失败，应该是超算做了下载文件大小的限制（2025.3更新，可以通过使用终端命令来上传和下载文件，没有文件大小限制）。<br>本内容仅针对pytorch和JAX环境的超算使用和安装，tensorflow用户可以借鉴使用（因为tensorflow往往要cuda和tf版本一一对应，有些时候可能不是环境安装问题而是版本对应问题），其他大语言模型机器学习包（例如libcuda.so）可能对cuda的版本和cuda driver的版本有要求，请联系管理员更新cuda和cuda driver。</p><span id="more"></span><h2 id="1-使用conda和linux系统离线打包完整conda环境"><a href="#1-使用conda和linux系统离线打包完整conda环境" class="headerlink" title="1 使用conda和linux系统离线打包完整conda环境"></a>1 使用conda和linux系统离线打包完整conda环境</h2><h3 id="1-1-使用conda-pack打包完整的conda环境"><a href="#1-1-使用conda-pack打包完整的conda环境" class="headerlink" title="1.1 使用conda-pack打包完整的conda环境"></a>1.1 使用<code>conda-pack</code>打包完整的conda环境</h3><p><strong>1.在联网机器上创建环境并打包</strong><br>首先你需要一个linux系统，并安装配置好conda<br>在可以联网的机器上，创建并安装完 <code>environment.yml</code> 或   <code>requirements.txt</code>中的依赖包后，使用 <code>conda-pack</code> 打包整个环境：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda activate my_envconda <span class="token function">install</span> conda-packconda-pack <span class="token parameter variable">-n</span> my_env <span class="token parameter variable">-o</span> my_env.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>2.超算解压</strong><br>在超算上可以在任意地址解压当前上传的环境包</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">tar</span> <span class="token parameter variable">-xzf</span> pytorch_env.tar.gz <span class="token parameter variable">-C</span> /path/to/env/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这里的&#x2F;path&#x2F;to&#x2F;env&#x2F;指的是自己设的解压地址，你也可以按我的直接在~&#x2F;pytorch_env</p><p><strong>3.云工具jupyterlab中激活环境</strong></p><p>此时已经可以在云工具的jupyterlab中通过jupyterlab 的terminal在环境解压后的 <code>bin</code> 目录中使用 <code>source</code> 命令激活环境：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">source</span> /path/to/env/bin/activate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/images/%E7%A6%BB%E7%BA%BF%E8%B6%85%E7%AE%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/image.png" class="lazyload" data-srcset="/images/%E7%A6%BB%E7%BA%BF%E8%B6%85%E7%AE%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/image.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="alt text"><br>上图的情况说明终端的环境加载成功<br>如果你使用的是网页版的超算，这个时候已经可以通过python命令运行py文件</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python main.py <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>对于更一般的情况，建议结合vscode和跳板机联合使用</p><h3 id="1-2-结合vscode的ssh多级跳跃本地化运行环境管理"><a href="#1-2-结合vscode的ssh多级跳跃本地化运行环境管理" class="headerlink" title="1.2 结合vscode的ssh多级跳跃本地化运行环境管理"></a>1.2 结合vscode的ssh多级跳跃本地化运行环境管理</h3><p><strong>这里需要你的ip地址能成功ssh到超算，不能的话找老师申请vpn</strong><br>在本地电脑上安装vscode和Remote-SSH插件<br><img src="/images/%E7%A6%BB%E7%BA%BF%E8%B6%85%E7%AE%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/image-1.png" class="lazyload" data-srcset="/images/%E7%A6%BB%E7%BA%BF%E8%B6%85%E7%AE%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/image-1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="alt text"><br>按照图上的形式进入Remote-SSH的配置文件<br>进入后输入</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 定义跳板机（Jump Host）</span>Host jump  HostName 集群ip地址  User 账号名<span class="token comment"># 定义目标主机gpu1，通过 jump 进行跳转，并做本地端口转发</span>Host gpu1<span class="token punctuation">(</span>任意自定义名<span class="token punctuation">)</span>  HostName 目标主机节点名  User 账号名  ProxyJump jump  LocalForward <span class="token number">8888</span> localhost<span class="token punctuation">:</span><span class="token number">8888</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>目标主机节点名可以通过登录集群后用sinfo查看gpu和cpu资源空闲状态确定<br>User改成你自己，端口号你输入8888 8889 8890 8899这种都可以<br>申请交互式计算节点</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">salloc <span class="token parameter variable">-p</span> <span class="token operator">&lt;</span>分区名<span class="token operator">></span> <span class="token parameter variable">-N</span> <span class="token operator">&lt;</span>节点数<span class="token operator">></span> <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>任务数<span class="token operator">></span> --cpus-per-task<span class="token operator">=</span><span class="token operator">&lt;</span>cpu数量<span class="token operator">></span> <span class="token parameter variable">--gres</span><span class="token operator">=</span><span class="token operator">&lt;</span>gpu数量<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在vscode中点击连接gpu1节点<br>此时在终端通过以下代码激活环境既可以用python运行py文件</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">source</span> /path/to/env/bin/activate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果你已经安装了jupyter notebook<br>在vscode里安装jupyter插件，你会看到在jupyter文件的右上角可以选择kernel<br>这个时候已经有了你解压的环境，可以方便的使用jupyter notebook进行代码训练</p><h2 id="2-slurm常用命令与使用方法"><a href="#2-slurm常用命令与使用方法" class="headerlink" title="2 slurm常用命令与使用方法"></a>2 slurm常用命令与使用方法</h2><p>查看集群分区与节点状态</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sinfo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看特定分区空闲节点<br>申请交互式计算节点<br>此方法个人感觉为最佳方法</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">salloc <span class="token parameter variable">-p</span> <span class="token operator">&lt;</span>分区名<span class="token operator">></span> <span class="token parameter variable">-N</span> <span class="token operator">&lt;</span>节点数<span class="token operator">></span> <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span>任务数<span class="token operator">></span> --cpus-per-task<span class="token operator">=</span><span class="token operator">&lt;</span>cpu数量<span class="token operator">></span> <span class="token parameter variable">--gres</span><span class="token operator">=</span><span class="token operator">&lt;</span>gpu数量<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下面这个为申请gpu节点的命令，申请一张gpu卡，同时保证关闭终端不会自动结束任务<br>（因为目前集群是免费的，太多人占卡了，占卡时长和你的电脑开机时间息息相关。。）<br>（使用此命令可以无限时长占卡）  </p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">salloc --no-kill <span class="token parameter variable">-p</span> gpu <span class="token parameter variable">--gres</span><span class="token operator">=</span>gpu:1  --job-name<span class="token operator">=</span>余翔洋  <span class="token operator">&amp;</span> disown<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>ssh 进入节点</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ssh</span> <span class="token operator">&lt;</span>账号名<span class="token operator">></span>@<span class="token operator">&lt;</span>节点名<span class="token operator">></span><span class="token function">ssh</span> <span class="token operator">&lt;</span>节点名<span class="token operator">></span>   <span class="token comment"># 如果已经登录集群</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>提交批处理作业</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sbatch <span class="token operator">&lt;</span>作业脚本文件名<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看作业状态</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">squeue <span class="token parameter variable">-u</span> <span class="token operator">&lt;</span>账号名<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>停止作业</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scancel <span class="token operator">&lt;</span>作业ID<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除作业</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scancel --delete-all <span class="token operator">&lt;</span>作业ID<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-slurm作业脚本示例"><a href="#3-slurm作业脚本示例" class="headerlink" title="3 slurm作业脚本示例"></a>3 slurm作业脚本示例</h2><blockquote><p>示例一</p></blockquote><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span><span class="token comment">#SBATCH --job-name='Common_Job_08281120' #定义工作名，任意名称即可</span><span class="token comment">#SBATCH --chdir=/share/home/task/slurm_task #定义脚本工作地址</span><span class="token comment">#SBATCH --partition=gpu_small #定义节点partition，选择gpu或者cpu分区</span><span class="token comment">#SBATCH --nodes=1 #节点数量，gpu的话一般都是1吧</span><span class="token comment">#SBATCH --ntasks-per-node=16 #cpu数量16</span><span class="token comment">#SBATCH --time=0-24:00 #运行时间24h</span><span class="token comment">#SBATCH --gres=gpu:2g.20gb:1 #因为是在small_gpu里，所以--gres=gpu:(抓取)2g.20gb（型号）:1(1个)</span><span class="token comment">#SBATCH --mail-type=BEGIN,END,FAIL #运行开始完毕、失败以后发送邮件</span><span class="token comment">#SBATCH --mail-user=yuxiangyang@zju.edu.cn #邮件地址</span><span class="token builtin class-name">echo</span> job start <span class="token function">time</span> is <span class="token variable"><span class="token variable">`</span><span class="token function">date</span><span class="token variable">`</span></span><span class="token builtin class-name">echo</span> <span class="token variable"><span class="token variable">`</span><span class="token function">hostname</span><span class="token variable">`</span></span><span class="token builtin class-name">source</span> ~/jupyter_pytorch_env/bin/activate <span class="token comment">#激活你的python环境，这里按第一章内容设置</span>python main.py <span class="token comment">#运行python文件</span><span class="token builtin class-name">echo</span> job end <span class="token function">time</span> is <span class="token variable"><span class="token variable">`</span><span class="token function">date</span><span class="token variable">`</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>示例二</p></blockquote><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span><span class="token comment">#SBATCH --job-name='Common_Job_08281109'</span><span class="token comment">#SBATCH --chdir=/share/home/22312004/task/slurm_task</span><span class="token comment">#SBATCH --partition=gpu</span><span class="token comment">#SBATCH --nodes=1</span><span class="token comment">#SBATCH --ntasks-per-node=16</span><span class="token comment">#SBATCH --time=0-24:00</span><span class="token comment">#SBATCH --gres=gpu:1 #因为是gpu节点，所以--gres=gpu:1即可，</span><span class="token builtin class-name">echo</span> job start <span class="token function">time</span> is <span class="token variable"><span class="token variable">`</span><span class="token function">date</span><span class="token variable">`</span></span><span class="token builtin class-name">echo</span> <span class="token variable"><span class="token variable">`</span><span class="token function">hostname</span><span class="token variable">`</span></span><span class="token builtin class-name">source</span> ~/jupyter_pytorch_env/bin/activatepython main.py<span class="token builtin class-name">echo</span> job end <span class="token function">time</span> is <span class="token variable"><span class="token variable">`</span><span class="token function">date</span><span class="token variable">`</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>写好的slurm脚本命名为run.slurm<br>可以在网页端的General Job里提交<br>也可以用命令提交</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sbatch run.slurm<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>运行成功后终端会显示</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Submitted batch job <span class="token number">12345678</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以通过<code>squeue -u 账号名</code>查看作业状态</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">JOBID PARTITION     NAME     <span class="token environment constant">USER</span> ST       TIME  NODES NODELIST<span class="token punctuation">(</span>REASON<span class="token punctuation">)</span> <span class="token number">12345678</span> gpu_small  Common_Job_08281120 yuxiangyang  R       <span class="token number">0</span>:01      <span class="token number">1</span> gpu-node<span class="token punctuation">[</span>01-02<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 环境配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 工程项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PINNs物理内嵌神经网络入门-1</title>
      <link href="/2025/10/29/pinn001/"/>
      <url>/2025/10/29/pinn001/</url>
      
        <content type="html"><![CDATA[<p>本篇字数约1600字，参考文献论文引用次数8845次，主要任务为介绍PINN的优势和PINN神经网络的结构组成与内部求导的理论实现。</p><span id="more"></span>]]></content>
      
      
      <categories>
          
          <category> 数值计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> neural network </tag>
            
            <tag> PINN </tag>
            
            <tag> 数值计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PINN 求解饱和土Biot固结</title>
      <link href="/2025/10/28/pinn-biot001/"/>
      <url>/2025/10/28/pinn-biot001/</url>
      
        <content type="html"><![CDATA[<p>这里后面会写更学术一点的摘要、方程、数值策略等。等待更新</p><span id="more"></span>]]></content>
      
      
      <categories>
          
          <category> 研究 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PINN </tag>
            
            <tag> Biot </tag>
            
            <tag> 多场耦合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全连接神经网络入门</title>
      <link href="/2025/10/28/fcn001/"/>
      <url>/2025/10/28/fcn001/</url>
      
        <content type="html"><![CDATA[<p>作为物理内嵌神经网络的预备知识和各种神经网络算法的鼻祖，学习全连接神经网络（Fully Connected Neural Network, FCNN）是非常有必要的。本篇文章将介绍全连接神经网络的基本结构、前向传播和反向传播算法，帮助读者理解神经网络的工作原理。</p><span id="more"></span><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2>]]></content>
      
      
      <categories>
          
          <category> neural network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> neural network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>钢箱梁施工设计与ansys动态模拟</title>
      <link href="/2024/01/07/ansys001/"/>
      <url>/2024/01/07/ansys001/</url>
      
        <content type="html"><![CDATA[<p>2024.1项目已完结<br>内容目前仅有图片，具体内容空闲了再补</p><span id="more"></span><p><img src="/images/%E9%92%A2%E7%AE%B1%E6%A2%81%E6%96%BD%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%B8%8Eansys%E5%8A%A8%E6%80%81%E6%A8%A1%E6%8B%9F/1.png" class="lazyload" data-srcset="/images/%E9%92%A2%E7%AE%B1%E6%A2%81%E6%96%BD%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%B8%8Eansys%E5%8A%A8%E6%80%81%E6%A8%A1%E6%8B%9F/1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="辅助支架设计"><br><img src="/images/%E9%92%A2%E7%AE%B1%E6%A2%81%E6%96%BD%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%B8%8Eansys%E5%8A%A8%E6%80%81%E6%A8%A1%E6%8B%9F/2.png" class="lazyload" data-srcset="/images/%E9%92%A2%E7%AE%B1%E6%A2%81%E6%96%BD%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%B8%8Eansys%E5%8A%A8%E6%80%81%E6%A8%A1%E6%8B%9F/2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="网格设计"><br><img src="/images/%E9%92%A2%E7%AE%B1%E6%A2%81%E6%96%BD%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%B8%8Eansys%E5%8A%A8%E6%80%81%E6%A8%A1%E6%8B%9F/3.png" class="lazyload" data-srcset="/images/%E9%92%A2%E7%AE%B1%E6%A2%81%E6%96%BD%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%B8%8Eansys%E5%8A%A8%E6%80%81%E6%A8%A1%E6%8B%9F/3.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="施工模拟"></p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工程项目 </tag>
            
            <tag> 桥梁施工模拟 </tag>
            
            <tag> ansys </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
